{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61b664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce85786",
   "metadata": {},
   "source": [
    "### Weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3ffbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomInitializer():        \n",
    "    def initialize(self, shape):\n",
    "        W = np.random.randn(shape[0], shape[1])\n",
    "        return W\n",
    "    \n",
    "class ZerosInitializer():\n",
    "    def initialize(self, shape):\n",
    "        W = np.zeros(shape)\n",
    "        return W\n",
    "\n",
    "class HeInitializer():\n",
    "    def initialize(self, shape):\n",
    "        W = np.random.randn(shape[0], shape[1]) * np.sqrt(2 / shape[1])\n",
    "        return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47a59c",
   "metadata": {},
   "source": [
    "## Activation funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f5e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RELU():\n",
    "    def activate(self, Z):\n",
    "        return Z * (Z > 0)\n",
    "    \n",
    "    def derivative(self, Z):\n",
    "        return 1 * (Z > 0)\n",
    "\n",
    "\n",
    "class Sigmoid():\n",
    "    def activate(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def derivative(self, Z):\n",
    "        return self.activate(Z) * (1 - self.activate(Z))\n",
    "    \n",
    "\n",
    "class Linear():\n",
    "    def activate(self, Z):\n",
    "        return Z\n",
    "    \n",
    "    def derivative(self, Z):\n",
    "        return (np.ones(Z.shape))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafefba",
   "metadata": {},
   "source": [
    "## Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d40d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy():\n",
    "    def compute_cost(self, y_pred, y_true):\n",
    "        #shape y_pred and y_true = (1, m_examples)\n",
    "        m = y_true.shape[1]\n",
    "        \n",
    "        #lets cut off a  tiny constant to avoid log0 problem\n",
    "        epsilon = 10 ** -15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "        \n",
    "        cost =  -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        \n",
    "        cost = np.sum(cost, axis=1, keepdims=True) * (1 / m)\n",
    "        \n",
    "        return (cost)\n",
    "    \n",
    "    def derivative(self, y_pred, y_true):\n",
    "        \n",
    "        #Do it to avoid division by 0\n",
    "        epsilon = 10 ** -15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "        \n",
    "        dA = - (y_true / y_pred) + (1 - y_true) / (1 - y_pred)\n",
    "        \n",
    "        return (dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0793ce8",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b47dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, n_units, activation, l2_reg=0, weight_initializer=HeInitializer):\n",
    "        self.activation = activation\n",
    "        self.n_units = n_units\n",
    "        self.l2_reg =l2_reg\n",
    "        \n",
    "        self.activation = activation()\n",
    "        \n",
    "        #initialize cache\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "        \n",
    "        #initialize params\n",
    "        #waiting for initializing the model\n",
    "        self.initializer = weight_initializer\n",
    "        self.W = None\n",
    "        self.b = ZerosInitializer().initialize((n_units, 1))\n",
    "        \n",
    "        #We will need them for Adam and Momentum\n",
    "        #moments\n",
    "        self.V_dW = None\n",
    "        self.V_db = ZerosInitializer().initialize((n_units, 1))\n",
    "        #RMS_prop part\n",
    "        self.S_dw = None \n",
    "        self.S_db = ZerosInitializer().initialize((n_units, 1))\n",
    "        \n",
    "        \n",
    "        #grads\n",
    "        self.dZ = None\n",
    "        self.dA = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def initialize(self, n_units_prev):\n",
    "        shape = (self.n_units, n_units_prev)\n",
    "        self.W = self.initializer().initialize(shape)\n",
    "        \n",
    "        #initialize params in case we use Adam/Momentum\n",
    "        self.V_dW = ZerosInitializer().initialize(shape)\n",
    "        self.S_dW = ZerosInitializer().initialize(shape)\n",
    "        \n",
    "        \n",
    "    def forward_propogation(self, A_prev):\n",
    "        #keep A_prev for backprop\n",
    "        self.A_prev = A_prev\n",
    "        \n",
    "        self.Z = np.dot(self.W, A_prev) + self.b\n",
    "        self.A = self.activation.activate(self.Z)\n",
    "        \n",
    "        return (self.A)\n",
    "    \n",
    "    def back_propogation(self, W_next=None, dZ_next=None, dA_final=None):\n",
    "        \n",
    "        batch_size = self.Z.shape[1]\n",
    "        \n",
    "        #Check for valid input\n",
    "        if dA_final is None:\n",
    "            if W_next is None or dZ_next is None:\n",
    "                raise ValueError(\"Either both W_next and dZ_next must be provided, or dA_final must be provided.\")\n",
    "\n",
    "        \n",
    "        #compute and keep gradients\n",
    "        #dA_final is a specific case, where our layer is final and we compute cost derivs\n",
    "        if dA_final is not None:\n",
    "            self.dA = dA_final\n",
    "        else:\n",
    "            self.dA = np.dot(W_next.T, dZ_next)\n",
    "            \n",
    "        self.dZ = self.dA * self.activation.derivative(self.Z)\n",
    "        \n",
    "        #regularization\n",
    "        l2_term = (self.l2_reg / batch_size) * self.W\n",
    "        \n",
    "        self.dW = np.dot(self.dZ, self.A_prev.T) * (1 / batch_size) + l2_term\n",
    "        self.db = np.sum(self.dZ, axis=1, keepdims=True) * (1 / batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa11ee",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b330b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent():\n",
    "    def __init__(self, learning_rate):\n",
    "        self.counter = 1\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, layer):\n",
    "        #update params of layer\n",
    "        layer.W = layer.W - self.learning_rate * layer.dW\n",
    "        layer.b = layer.b - self.learning_rate * layer.db\n",
    "        \n",
    "    def tick(self):\n",
    "        self.counter += 1\n",
    "        \n",
    "\n",
    "class Momentum():\n",
    "    def __init__(self, learning_rate, beta=0.9, bias_correction=False):\n",
    "        \n",
    "        self.counter = 1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.bias_correction = bias_correction\n",
    "        self.epsilon = 10 ** -8\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \n",
    "        #compute new velocities\n",
    "        layer.V_dW = self.beta * layer.V_dW + (1 - self.beta) * layer.dW\n",
    "        layer.V_db = self.beta * layer.V_db + (1 - self.beta) * layer.db\n",
    "        \n",
    "        if self.bias_correction:\n",
    "            #correct velocities\n",
    "            layer.V_dW = layer.V_dW / (1 - self.beta ** self.counter)\n",
    "            layer.V_db = layer.V_db / (1 - self.beta ** self.counter)\n",
    "        \n",
    "        \n",
    "        #update params\n",
    "        layer.W = layer.W - self.learning_rate * layer.V_dW\n",
    "        layer.b = layer.b - self.learning_rate * layer.V_db\n",
    "        \n",
    "    def tick(self):\n",
    "        self.counter += 1\n",
    "        \n",
    "\n",
    "class Adam():\n",
    "    def __init__(self, alpha, beta1=0.9, beta2=0.99, bias_correction=False):\n",
    "        \n",
    "        self.counter = 1\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1 #Momentum\n",
    "        self.beta2 = beta2 #RMSprop\n",
    "        \n",
    "        self.bias_correction = bias_correction\n",
    "        \n",
    "        self.epsilon = 10 ** -8\n",
    "        \n",
    "    def update(self, layer):\n",
    "        #compute new velocities\n",
    "        layer.V_dW = self.beta1 * layer.V_dW + (1 - self.beta1) * layer.dW\n",
    "        layer.V_db = self.beta1 * layer.V_db + (1 - self.beta1) * layer.db\n",
    "        \n",
    "        #compute new second moments\n",
    "        layer.S_dW = self.beta2 * layer.S_dW + (1 - self.beta2) * np.square(layer.dW)\n",
    "        layer.S_db = self.beta2 * layer.S_db + (1 - self.beta2) * np.square(layer.db)\n",
    "        \n",
    "        \n",
    "        if self.bias_correction:\n",
    "            #correct velocities\n",
    "            layer.V_dW = layer.V_dW / (1 - self.beta1 ** self.counter)\n",
    "            layer.V_db = layer.V_db / (1 - self.beta1 ** self.counter)\n",
    "            \n",
    "            #correct 2nd moments\n",
    "            layer.S_dW = layer.S_dW / (1 - self.beta2 ** self.counter)\n",
    "            layer.S_db = layer.S_db / (1 - self.beta2 ** self.counter)\n",
    "            \n",
    "        #UPdate parameters\n",
    "        \n",
    "        layer.W = layer.W - self.alpha * (layer.V_dW / (np.sqrt(layer.S_dW) + self.epsilon))\n",
    "        layer.b = layer.b - self.alpha * (layer.V_db / (np.sqrt(layer.S_db) + self.epsilon))\n",
    "        \n",
    "    def tick(self):\n",
    "        self.counter += 1 \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4dd48984",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Layer(3, activation=Sigmoid)\n",
    "\n",
    "\n",
    "X = np.array([[3, 4],\n",
    "              [0, 3],\n",
    "               [2, 4],\n",
    "               [0 , 0],\n",
    "                [3, 0]]).T\n",
    "\n",
    "y = np.array([[1, 0, 1, 0, 1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dbff54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, cost = [[0.68381745]]\n",
      "i = 10, cost = [[0.56138149]]\n",
      "i = 20, cost = [[0.47808063]]\n",
      "i = 30, cost = [[0.41353049]]\n",
      "i = 40, cost = [[0.34077586]]\n",
      "i = 50, cost = [[0.26387362]]\n",
      "i = 60, cost = [[0.19297757]]\n",
      "i = 70, cost = [[0.13953807]]\n",
      "i = 80, cost = [[0.11494505]]\n",
      "i = 90, cost = [[0.09826149]]\n",
      "i = 100, cost = [[0.08696589]]\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer(n_units=3, activation = RELU)\n",
    "layer2 = Layer(1, Sigmoid)\n",
    "costf = BinaryCrossEntropy()\n",
    "\n",
    "layer1.initialize(2)\n",
    "layer2.initialize(3)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.01\n",
    "epochs = 100\n",
    "\n",
    "optimizer = Adam(alpha)\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    a1 = layer1.forward_propogation(X)\n",
    "    a2 = layer2.forward_propogation(a1)\n",
    "\n",
    "    cost = costf.compute_cost(a2, y)\n",
    "    if (i % (epochs//10) == 0):\n",
    "        print(\"i = {}, cost = {}\".format(i, cost))\n",
    "    \n",
    "    \n",
    "    dA_final = costf.derivative(a2, y)\n",
    "\n",
    "    #back_prop\n",
    "    layer2.back_propogation(dA_final=dA_final)\n",
    "    layer1.back_propogation(layer2.W, layer2.dZ)\n",
    "    \n",
    "    #update params\n",
    "    optimizer.update(layer1)\n",
    "    optimizer.update(layer2)\n",
    "    optimizer.tick()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "154d51d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.39771970e-103, 1.62133231e-061, 1.14811064e-107,\n",
       "        1.00000000e+000, 1.00000000e+000]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = layer1.forward_propogation(X)\n",
    "a2 = layer2.forward_propogation(a1)\n",
    "\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a82e2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71158021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.66965741,  0.0046513 ],\n",
       "       [-0.03837817, -0.28983827],\n",
       "       [-1.48759324,  0.12612852]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520521b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
