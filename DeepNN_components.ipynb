{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61b664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce85786",
   "metadata": {},
   "source": [
    "### Weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3ffbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomInitializer():        \n",
    "    def initialize(self, shape):\n",
    "        W = np.random.randn(shape[0], shape[1])\n",
    "        return W\n",
    "    \n",
    "class ZerosInitializer():\n",
    "    def initialize(self, shape):\n",
    "        W = np.zeros(shape)\n",
    "        return W\n",
    "\n",
    "class HeInitializer():\n",
    "    def initialize(self, shape):\n",
    "        W = np.random.randn(shape[0], shape[1]) * np.sqrt(2 / shape[1])\n",
    "        return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63e8c6",
   "metadata": {},
   "source": [
    "## Activation funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0afeef23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.19661193, 0.00664806, 0.04124902]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RELU():\n",
    "    def activate(self, Z):\n",
    "        return Z * (Z > 0)\n",
    "    \n",
    "    def derivative(self, Z):\n",
    "        return 1 * (Z > 0)\n",
    "\n",
    "\n",
    "class Sigmoid():\n",
    "    def activate(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def derivative(self, Z):\n",
    "        return self.activate(Z) * (1 - self.activate(Z))\n",
    "    \n",
    "\n",
    "class Linear():\n",
    "    def activate(self, Z):\n",
    "        return Z\n",
    "    \n",
    "    def derivative(self, Z):\n",
    "        return (np.ones(Z.shape))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0793ce8",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b47dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, n_units, activation, l2_reg=0, weight_initializer=HeInitializer):\n",
    "        self.activation = activation\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        \n",
    "        self.activation_function = activation()\n",
    "        \n",
    "        #initialize cache\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "        \n",
    "        #initialize params\n",
    "        #waiting for initializing the model\n",
    "        self.initializer = weight_initializer\n",
    "        self.W = None\n",
    "        self.b = ZerosInitializer().initialize((n_units, 1))\n",
    "        \n",
    "        #grads\n",
    "        self.dZ = None\n",
    "        self.dA = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def initialize(self, n_units_prev):\n",
    "        shape = (self.n_units, n_units_prev)\n",
    "        self.W = self.initializer().initialize(shape)\n",
    "        \n",
    "        \n",
    "    def forward_propogation(self, A_prev):\n",
    "        #keep A_prev for backprop\n",
    "        self.A_prev = A_prev\n",
    "        \n",
    "        self.Z = np.dot(self.W, A_prev) + self.b\n",
    "        self.A = self.activation.activate(self.Z)\n",
    "        \n",
    "        return (self.A)\n",
    "    \n",
    "    def back_propogation(self, W_next, dZ_next):\n",
    "        \n",
    "        batch_size = self.Z.shape[1]\n",
    "        \n",
    "        #compute and keep gradients\n",
    "        self.dA = np.dot(W_next.T, dZ_next)\n",
    "        self.dZ = self.dA * self.activation.derivative(self.Z)\n",
    "        \n",
    "        #regularization\n",
    "        l2_term = (self.l2_reg / batch_size) * self.W\n",
    "        \n",
    "        self.dW = np.dot(self.dZ, self.A_prev.T) * (1 / batch_size) + l2_term\n",
    "        self.db = np.sum(self.dZ, axis=1, keepdims=True) * (1 / batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4dd48984",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Layer(3, activation=Sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669c7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
