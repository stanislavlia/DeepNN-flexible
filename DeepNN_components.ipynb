{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c61b664e",
      "metadata": {
        "id": "c61b664e"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce85786",
      "metadata": {
        "id": "7ce85786"
      },
      "source": [
        "### Weight initializers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bc3ffbe6",
      "metadata": {
        "id": "bc3ffbe6"
      },
      "outputs": [],
      "source": [
        "class RandomInitializer():\n",
        "    def initialize(self, shape):\n",
        "        W = np.random.randn(shape[0], shape[1])\n",
        "        return W\n",
        "\n",
        "class ZerosInitializer():\n",
        "    def initialize(self, shape):\n",
        "        W = np.zeros(shape)\n",
        "        return W\n",
        "\n",
        "class HeInitializer():\n",
        "    def initialize(self, shape):\n",
        "        W = np.random.randn(shape[0], shape[1]) * np.sqrt(2 / shape[1])\n",
        "        return W\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b47a59c",
      "metadata": {
        "id": "9b47a59c"
      },
      "source": [
        "## Activation funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "98f5e669",
      "metadata": {
        "id": "98f5e669"
      },
      "outputs": [],
      "source": [
        "class RELU():\n",
        "    def activate(self, Z):\n",
        "        return Z * (Z > 0)\n",
        "\n",
        "    def derivative(self, Z):\n",
        "        return 1 * (Z > 0)\n",
        "\n",
        "\n",
        "class Sigmoid():\n",
        "    def activate(self, Z):\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "    def derivative(self, Z):\n",
        "        return self.activate(Z) * (1 - self.activate(Z))\n",
        "\n",
        "\n",
        "class Linear():\n",
        "    def activate(self, Z):\n",
        "        return Z\n",
        "\n",
        "    def derivative(self, Z):\n",
        "        return (np.ones(Z.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaafefba",
      "metadata": {
        "id": "aaafefba"
      },
      "source": [
        "## Costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b5d40d62",
      "metadata": {
        "id": "b5d40d62"
      },
      "outputs": [],
      "source": [
        "class BinaryCrossEntropy():\n",
        "    def compute_cost(self, y_pred, y_true):\n",
        "        #shape y_pred and y_true = (1, m_examples)\n",
        "        m = y_true.shape[1]\n",
        "\n",
        "        #lets cut off a  tiny constant to avoid log0 problem\n",
        "        epsilon = 10 ** -15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
        "\n",
        "        cost =  -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "        cost = np.sum(cost, axis=1, keepdims=True) * (1 / m)\n",
        "\n",
        "        return (cost)\n",
        "\n",
        "    def derivative(self, y_pred, y_true):\n",
        "\n",
        "        #Do it to avoid division by 0\n",
        "        epsilon = 10 ** -15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
        "\n",
        "        dA = - (y_true / y_pred) + (1 - y_true) / (1 - y_pred)\n",
        "\n",
        "        return (dA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0793ce8",
      "metadata": {
        "id": "b0793ce8"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b47dcaf",
      "metadata": {
        "id": "8b47dcaf"
      },
      "outputs": [],
      "source": [
        "class Layer():\n",
        "    def __init__(self, n_units, activation, l2_reg=0, weight_initializer=HeInitializer):\n",
        "        self.activation = activation\n",
        "        self.n_units = n_units\n",
        "        self.l2_reg =l2_reg\n",
        "\n",
        "        self.activation = activation()\n",
        "\n",
        "        #initialize cache\n",
        "        self.Z = None\n",
        "        self.A = None\n",
        "\n",
        "        #initialize params\n",
        "        #waiting for initializing the model\n",
        "        self.initializer = weight_initializer\n",
        "        self.W = None\n",
        "        self.b = ZerosInitializer().initialize((n_units, 1))\n",
        "\n",
        "        #We will need them for Adam and Momentum\n",
        "        #moments\n",
        "        self.V_dW = None\n",
        "        self.V_db = ZerosInitializer().initialize((n_units, 1))\n",
        "        #RMS_prop part\n",
        "        self.S_dw = None\n",
        "        self.S_db = ZerosInitializer().initialize((n_units, 1))\n",
        "\n",
        "\n",
        "        #grads\n",
        "        self.dZ = None\n",
        "        self.dA = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def initialize(self, n_units_prev):\n",
        "        shape = (self.n_units, n_units_prev)\n",
        "        self.W = self.initializer().initialize(shape)\n",
        "\n",
        "        #initialize params in case we use Adam/Momentum\n",
        "        self.V_dW = ZerosInitializer().initialize(shape)\n",
        "        self.S_dW = ZerosInitializer().initialize(shape)\n",
        "\n",
        "\n",
        "    def forward_propogation(self, A_prev):\n",
        "        #keep A_prev for backprop\n",
        "        self.A_prev = A_prev\n",
        "\n",
        "        self.Z = np.dot(self.W, A_prev) + self.b\n",
        "        self.A = self.activation.activate(self.Z)\n",
        "\n",
        "        return (self.A)\n",
        "\n",
        "    def back_propogation(self, W_next=None, dZ_next=None, dA_final=None):\n",
        "\n",
        "        batch_size = self.Z.shape[1]\n",
        "\n",
        "        #Check for valid input\n",
        "        if dA_final is None:\n",
        "            if W_next is None or dZ_next is None:\n",
        "                raise ValueError(\"Either both W_next and dZ_next must be provided, or dA_final must be provided.\")\n",
        "\n",
        "\n",
        "        #compute and keep gradients\n",
        "        #dA_final is a specific case, where our layer is final and we compute cost derivs\n",
        "        if dA_final is not None:\n",
        "            self.dA = dA_final\n",
        "        else:\n",
        "            self.dA = np.dot(W_next.T, dZ_next)\n",
        "\n",
        "        self.dZ = self.dA * self.activation.derivative(self.Z)\n",
        "\n",
        "        #regularization\n",
        "        l2_term = (self.l2_reg / batch_size) * self.W\n",
        "\n",
        "        self.dW = np.dot(self.dZ, self.A_prev.T) * (1 / batch_size) + l2_term\n",
        "        self.db = np.sum(self.dZ, axis=1, keepdims=True) * (1 / batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67fa11ee",
      "metadata": {
        "id": "67fa11ee"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "b330b013",
      "metadata": {
        "id": "b330b013"
      },
      "outputs": [],
      "source": [
        "class GradientDescent():\n",
        "    def __init__(self, learning_rate):\n",
        "        self.counter = 1\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, layer):\n",
        "        #update params of layer\n",
        "        layer.W = layer.W - self.learning_rate * layer.dW\n",
        "        layer.b = layer.b - self.learning_rate * layer.db\n",
        "\n",
        "    def tick(self):\n",
        "        self.counter += 1\n",
        "\n",
        "\n",
        "class Momentum():\n",
        "    def __init__(self, learning_rate, beta=0.9, bias_correction=False):\n",
        "\n",
        "        self.counter = 1\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta = beta\n",
        "\n",
        "        self.bias_correction = bias_correction\n",
        "        self.epsilon = 10 ** -8\n",
        "\n",
        "    def update(self, layer):\n",
        "\n",
        "        #compute new velocities\n",
        "        layer.V_dW = self.beta * layer.V_dW + (1 - self.beta) * layer.dW\n",
        "        layer.V_db = self.beta * layer.V_db + (1 - self.beta) * layer.db\n",
        "\n",
        "        if self.bias_correction:\n",
        "            #correct velocities\n",
        "            layer.V_dW = layer.V_dW / (1 - self.beta ** self.counter)\n",
        "            layer.V_db = layer.V_db / (1 - self.beta ** self.counter)\n",
        "\n",
        "\n",
        "        #update params\n",
        "        layer.W = layer.W - self.learning_rate * layer.V_dW\n",
        "        layer.b = layer.b - self.learning_rate * layer.V_db\n",
        "\n",
        "    def tick(self):\n",
        "        self.counter += 1\n",
        "\n",
        "\n",
        "class Adam():\n",
        "    def __init__(self, learning_rate, beta1=0.9, beta2=0.99, bias_correction=False):\n",
        "\n",
        "        self.counter = 1\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1 #Momentum\n",
        "        self.beta2 = beta2 #RMSprop\n",
        "\n",
        "        self.bias_correction = bias_correction\n",
        "\n",
        "        self.epsilon = 10 ** -8\n",
        "\n",
        "    def update(self, layer):\n",
        "        #compute new velocities\n",
        "        layer.V_dW = self.beta1 * layer.V_dW + (1 - self.beta1) * layer.dW\n",
        "        layer.V_db = self.beta1 * layer.V_db + (1 - self.beta1) * layer.db\n",
        "\n",
        "        #compute new second moments\n",
        "        layer.S_dW = self.beta2 * layer.S_dW + (1 - self.beta2) * np.square(layer.dW)\n",
        "        layer.S_db = self.beta2 * layer.S_db + (1 - self.beta2) * np.square(layer.db)\n",
        "\n",
        "\n",
        "        if self.bias_correction:\n",
        "            #correct velocities\n",
        "            layer.V_dW = layer.V_dW / (1 - self.beta1 ** self.counter)\n",
        "            layer.V_db = layer.V_db / (1 - self.beta1 ** self.counter)\n",
        "\n",
        "            #correct 2nd moments\n",
        "            layer.S_dW = layer.S_dW / (1 - self.beta2 ** self.counter)\n",
        "            layer.S_db = layer.S_db / (1 - self.beta2 ** self.counter)\n",
        "\n",
        "        #UPdate parameters\n",
        "\n",
        "        layer.W = layer.W - self.learning_rate * (layer.V_dW / (np.sqrt(layer.S_dW) + self.epsilon))\n",
        "        layer.b = layer.b - self.learning_rate * (layer.V_db / (np.sqrt(layer.S_db) + self.epsilon))\n",
        "\n",
        "    def tick(self):\n",
        "        self.counter += 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models**"
      ],
      "metadata": {
        "id": "Xdy6ocm6ylPu"
      },
      "id": "Xdy6ocm6ylPu"
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "  def __init__(self, X, Y,layers):\n",
        "    #X has shape (n_features, m_examples)\n",
        "    #Y has shape (1, m_examples)\n",
        "\n",
        "    self.layers = layers\n",
        "    self.n_layers = len(layers)\n",
        "    self.m_examples = X.shape[1]\n",
        "\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "    self.minibatches = [] #list of tuples (X_batch, Y_batch)\n",
        "\n",
        "    self.costfunc = None\n",
        "    self.optimizer = None\n",
        "\n",
        "  def _init_weights(self):\n",
        "\n",
        "    n_unit_prev = self.X.shape[0]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      layer.initialize(n_unit_prev)\n",
        "\n",
        "      #set n_units_prev for next initialization\n",
        "      n_unit_prev = layer.n_units\n",
        "\n",
        "  def _make_minibatches(self, batch_size=None):\n",
        "    if batch_size is None:\n",
        "      self.minibatches.append((self.X, self.Y))\n",
        "      return\n",
        "\n",
        "    complete_batches = self.m_examples // batch_size\n",
        "\n",
        "    for k in range(0, complete_batches):\n",
        "\n",
        "        #extracting a particular slice of data\n",
        "        X_k = self.X[:, k * batch_size : (k + 1) * batch_size]\n",
        "        Y_k = self.Y[:, k * batch_size : (k + 1) * batch_size]\n",
        "\n",
        "        minibatch = (X_k, Y_k)\n",
        "        self.minibatches.append(minibatch)\n",
        "\n",
        "        #add incomplete batch in case we have remaining examples\n",
        "    if (self.m_examples % batch_size != 0):\n",
        "      X_k = self.X[:, complete_batches * batch_size : ]\n",
        "      Y_k = self.Y[:, complete_batches * batch_size : ]\n",
        "      minibatch = (X_k, Y_k)\n",
        "      self.minibatches.append(minibatch)\n",
        "\n",
        "\n",
        "\n",
        "    #TODO: implement splitting into minibatches here...\n",
        "\n",
        "  def compile(self, costfunc, optimizer):\n",
        "    self.costfunc = costfunc\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    A_prev = X\n",
        "\n",
        "    for layer in self.layers:\n",
        "      A = layer.forward_propogation(A_prev)\n",
        "      A_prev = A\n",
        "\n",
        "    return A\n",
        "\n",
        "  def _backprop(self, dA_final):\n",
        "\n",
        "      L = self.n_layers\n",
        "\n",
        "      #mannualy set backprop for last layer\n",
        "      self.layers[L - 1].back_propogation(dA_final=dA_final)\n",
        "\n",
        "      #loop goes from L - 2 up to 0\n",
        "      for l in range(L - 2, -1, -1):\n",
        "        self.layers[l].back_propogation(self.layers[l + 1].W,\n",
        "                                        self.layers[l + 1].dZ)\n",
        "\n",
        "  def _update_all_params(self):\n",
        "    #goes over layers updating params using computed gradients\n",
        "    for layer in self.layers:\n",
        "      self.optimizer.update(layer)\n",
        "\n",
        "    self.optimizer.tick()\n",
        "\n",
        "\n",
        "  def fit(self, epochs, batch_size=None, n_reports=10):\n",
        "\n",
        "    #init weights\n",
        "    self._init_weights()\n",
        "\n",
        "    #split into minibatches\n",
        "    self._make_minibatches(batch_size)\n",
        "\n",
        "\n",
        "    self.history = []\n",
        "    for epoch in range(0, epochs + 1):\n",
        "\n",
        "      #compute cost\n",
        "      all_predictions = self.predict(self.X)\n",
        "      total_cost = self.costfunc.compute_cost(all_predictions, self.Y)\n",
        "      self.history.append(float(total_cost))\n",
        "\n",
        "      if ((epoch) % (epochs // n_reports) == 0):\n",
        "        print(\"Epoch #{}, cost = {}\".format(epoch, total_cost))\n",
        "\n",
        "\n",
        "      for batch in self.minibatches:\n",
        "        X_batch, Y_batch = batch\n",
        "\n",
        "        batch_predictions = self.predict(X_batch)\n",
        "\n",
        "        dA_final = self.costfunc.derivative(batch_predictions, Y_batch)\n",
        "\n",
        "        self._backprop(dA_final)\n",
        "        self._update_all_params()\n"
      ],
      "metadata": {
        "id": "M2mkGxe_mA5r"
      },
      "id": "M2mkGxe_mA5r",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using our model on real data**\n",
        "\n",
        "So, let's upload titanic dataset"
      ],
      "metadata": {
        "id": "SSvZ-Vg049AP"
      },
      "id": "SSvZ-Vg049AP"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "d520521b",
      "metadata": {
        "id": "d520521b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Data Preprocessing***"
      ],
      "metadata": {
        "id": "iAK-2RNZ6cEy"
      },
      "id": "iAK-2RNZ6cEy"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"titanic_train.csv\")\n",
        "\n",
        "#some data definetly useless for predicion such as ticket number, name\n",
        "df = df.drop([\"Ticket\", \"PassengerId\", \"Name\", \"Cabin\"], axis=1)\n",
        "\n",
        "df[\"Sex\"] = df[\"Sex\"].replace({\"male\" : 0, \"female\" : 1})\n",
        "\n",
        "df[\"Embarked\"] = df[\"Embarked\"].replace({\"S\" : 0, \"C\" : 1, \"Q\": 2})\n",
        "\n",
        "\n",
        "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "X = df.drop(\"Survived\", axis=1).values\n",
        "y = df[\"Survived\"].values\n",
        "\n",
        "\n",
        "#Scaling Features\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "X_scaled = std_scaler.fit_transform(X)\n",
        "\n",
        "#Splitting into train/test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.75, random_state=1)\n",
        "\n",
        "#preparing for our model\n",
        "X_train = X_train.T\n",
        "y_train = y_train.reshape(1, -1)\n",
        "X_test = X_test.T\n",
        "y_test = y_test.reshape(1, -1)"
      ],
      "metadata": {
        "id": "iEcdAU666Zrn"
      },
      "id": "iEcdAU666Zrn",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's initialize our model"
      ],
      "metadata": {
        "id": "mWgqKz0K7BCV"
      },
      "id": "mWgqKz0K7BCV"
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [\n",
        "        Layer(n_units=15, activation=RELU, l2_reg=0.1),\n",
        "        Layer(n_units=10, activation=RELU, l2_reg=0.05),\n",
        "        Layer(n_units=4, activation=RELU, l2_reg=0.05),\n",
        "        Layer(n_units=1, activation=Sigmoid) #Final layer\n",
        "          ]\n",
        "\n",
        "\n",
        "model = Model(X_train, y_train, layers)\n",
        "\n",
        "model.compile(\n",
        "              costfunc=BinaryCrossEntropy(),\n",
        "              optimizer=Adam(learning_rate=0.01)\n",
        "              )"
      ],
      "metadata": {
        "id": "2moF76Y87DFQ"
      },
      "id": "2moF76Y87DFQ",
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(epochs=1000, batch_size=64, n_reports=10)"
      ],
      "metadata": {
        "id": "IMeWDKeK8iBX",
        "outputId": "82a70f44-eeca-4bad-b398-83b36d88d18a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IMeWDKeK8iBX",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0, cost = [[0.67735091]]\n",
            "Epoch #100, cost = [[0.29782911]]\n",
            "Epoch #200, cost = [[0.27492887]]\n",
            "Epoch #300, cost = [[0.26402751]]\n",
            "Epoch #400, cost = [[0.26172088]]\n",
            "Epoch #500, cost = [[0.25132628]]\n",
            "Epoch #600, cost = [[0.25247051]]\n",
            "Epoch #700, cost = [[0.24564107]]\n",
            "Epoch #800, cost = [[0.24687623]]\n",
            "Epoch #900, cost = [[0.24188319]]\n",
            "Epoch #1000, cost = [[0.25042308]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2e4EfTgD89uY"
      },
      "id": "2e4EfTgD89uY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}